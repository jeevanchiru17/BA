{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 100\n",
    "page_size = 100\n",
    "\n",
    "reviews = []\n",
    "\n",
    "# for i in range(1, pages + 1):\n",
    "for i in range(1, pages + 1):\n",
    "\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "    \n",
    "    print(f\"   ---> {len(reviews)} total reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviews']=df[\"reviews\"].str.replace('✅ Trip Verified |','')\n",
    "df['reviews']=df[\"reviews\"].str.replace('|','')\n",
    "df['reviews']=df[\"reviews\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['reviews'].str.contains('Not Verified')]\n",
    "\n",
    "# Resetting the index after filtering\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW WE DO SENTIMENT AND CONTEXTUAL ANALYASIS ON THE FOLLWING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "# Resetting the index with continuous numbers starting from 1\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the DataFrame after resetting the index\n",
    "print(df)\n",
    "df = df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    if isinstance(text, str):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                               u\"\\U00002500-\\U00002BEF\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df['reviews'] = df['reviews'].apply(remove_emojis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    else:\n",
    "        return text\n",
    "df['reviews']=df['reviews'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Function to perform spell checking\n",
    "def correct_spelling(text):\n",
    "    if isinstance(text, str):\n",
    "        spell = SpellChecker()\n",
    "        words = text.split()\n",
    "        corrected_words = [spell.correction(word) if word.isalpha() else word for word in words]\n",
    "        cleaned_words = [word for word in corrected_words if word is not None and isinstance(word, str)]\n",
    "        return ' '.join(cleaned_words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Take a smaller sample of the data (e.g., the first 1000 rows)\n",
    "sample_size = 1000\n",
    "sample = df['reviews'].iloc[:sample_size]\n",
    "\n",
    "# Apply the function to the sample\n",
    "df['reviews'].iloc[:sample_size] = sample.apply(correct_spelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviews'] = df['reviews'].apply(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZATON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources (if you haven't already)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the 'reviews' column\n",
    "df['reviews_tokens'] = df['reviews'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOPWORD REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords (if you haven't already)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove 'not' from the stop words set\n",
    "stop_words.discard('not')\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        return filtered_tokens\n",
    "    else:\n",
    "        return tokens\n",
    "\n",
    "# Apply stopword removal to the tokenized column\n",
    "df['reviews_tokens_without_stopwords'] = df['reviews_tokens'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Function to perform stemming\n",
    "def perform_stemming(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        stemmed_tokens = [porter.stem(word) for word in tokens]\n",
    "        return stemmed_tokens\n",
    "    else:\n",
    "        return tokens\n",
    "\n",
    "# Apply stemming to the tokens without stopwords\n",
    "df['reviews_stemmed'] = df['reviews_tokens_without_stopwords'].apply(perform_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to perform lemmatization\n",
    "def perform_lemmatization(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return lemmatized_tokens\n",
    "    else:\n",
    "        return tokens\n",
    "\n",
    "# Apply lemmatization to the stemmed tokens\n",
    "df['reviews_lemmatized'] = df['reviews_stemmed'].apply(perform_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to find sentiment polarity and label\n",
    "def find_sentiment(text):\n",
    "    if isinstance(text, list):\n",
    "        analysis = TextBlob(' '.join(text))\n",
    "        polarity = analysis.sentiment.polarity\n",
    "\n",
    "        # Classify sentiment based on polarity\n",
    "        if polarity > 0:\n",
    "            sentiment = 'Positive'\n",
    "        elif polarity < 0:\n",
    "            sentiment = 'Negative'\n",
    "        else:\n",
    "            sentiment = 'Neutral'\n",
    "\n",
    "        return polarity, sentiment\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Apply sentiment analysis and labeling\n",
    "df['sentiment_polarity'], df['sentiment'] = zip(*df['reviews_lemmatized'].apply(find_sentiment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('BAA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'sentiment_polarity' is the column in your DataFrame containing sentiment polarity scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['sentiment_polarity'], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Sentiment Polarity Scores')\n",
    "plt.xlabel('Sentiment Polarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'sentiment' is the column in your DataFrame indicating sentiment\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sentiment_counts.plot(kind='bar', color=['green', 'red', 'gray'])\n",
    "plt.title('Distribution of Sentiments')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', colors=['green', 'red', 'gray'])\n",
    "plt.title('Distribution of Sentiments')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'sentiment' is the column in your DataFrame indicating sentiment\n",
    "\n",
    "# Create separate DataFrames for positive, negative, and neutral sentiments\n",
    "positive_df = df[df['sentiment'] == 'Positive']\n",
    "negative_df = df[df['sentiment'] == 'Negative']\n",
    "neutral_df = df[df['sentiment'] == 'Neutral']\n",
    "\n",
    "# Save each DataFrame to a separate CSV file\n",
    "positive_df.to_csv('positive_reviews.csv', index=False)\n",
    "negative_df.to_csv('negative_reviews.csv', index=False)\n",
    "neutral_df.to_csv('neutral_reviews.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scipy==1.7.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file containing negative reviews\n",
    "negative_df = pd.read_csv('negative_reviews.csv')\n",
    "\n",
    "# Assuming 'reviews' is the column in your DataFrame containing the negative reviews\n",
    "negative_reviews = negative_df['reviews']\n",
    "\n",
    "# Find the top 5 repeated sentences\n",
    "top_repeated_sentences = negative_reviews.value_counts().head(5)\n",
    "\n",
    "# Display the top 5 repeated sentences\n",
    "print(\"Top 5 Repeated Sentences in Negative Reviews:\")\n",
    "print(top_repeated_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f7924c4c56b083e0e50eadfe7ef592a7a8ef70df33a0047f82280e6be1afe15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
